{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZLKAia_mfyIu"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KReDvi7eiRvf"},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.feature_selection import RFE\n","from operator import itemgetter\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn import tree"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1381,"status":"ok","timestamp":1686178114968,"user":{"displayName":"Katherine Marquis","userId":"17438712515933825923"},"user_tz":420},"id":"2CF0Otz0iWdt","outputId":"0aae930f-5148-4510-b115-daf26d629d5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","# path from your drive to data\n","drive.mount('/content/drive/')"]},{"cell_type":"markdown","metadata":{"id":"-K1nHn1xf6mJ"},"source":["After data frames of the averages of the last n games have been created, each dataframe is split into an input matrix X and a binary classification matrix y to perform Feature Selection and Machine Learning. Initially, we look at data where n = 1, 5, 10, 15, and 20 to see how feature selection methods may vary based\n","on n. We don't go above n=20 because we increase chances of model drift."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8gCHzECiJ6k"},"outputs":[],"source":["path = '/content/drive/MyDrive/College/Senior_Year/SP/BEM_Ec_120/Project/Womens/data/n_avg_matchups/'\n","\n","# n_data_dict[n][0]: dataframe of loaded csv; each row in data frame includes winner and loser stats\n","# n_data_dict[n][1]: X_pre_2023 (input for training data)\n","# n_data_dict[n][2]: y_pre_2023 (classification for training data)\n","# n_data_dict[n][3]: X_2023 (input for testing data)\n","# n_data_dict[n][4]: y_2023 (classification for testing data)\n","\n","n = {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,20,30}\n","n_data_dict = {}\n","\n","for key in n:\n","  n_data_dict[key] = []\n","  n_data_dict[key].append(pd.read_csv(path + 'Mavg_matchups_n={}.csv'.format(key), index_col=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bq1eK-pff6L8"},"outputs":[],"source":["cols = ['Season', 'Score', 'FGM', \n","        'FGA', 'FGp', 'FGM3', 'FGA3', 'FGp3', 'FTM', \n","        'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl',\n","        'Blk', 'PF', 'TSP', 'EFGp', '3PTAr', 'FTAr', \n","        'ORp', 'DRp', 'TRp', 'TR', 'Poss', 'OffRtg', 'DefRtg',\n","        'NetRtg', 'OffEff', 'DefEff', 'NetEff', \n","        'PointDiff', 'Pace','PtsPer100', 'Ar', 'TOr', \n","        'ATRatio', 'ELO', 'PythagoreanExp1', 'PythagoreanExp2']\n","\n","win_cols = ['Season', 'WScore',\n","            'WFGM', 'WFGA', 'WFGp',\n","            'WFGM3', 'WFGA3', 'WFGp3', 'WFTM',\n","            'WFTA', 'WOR', 'WDR', 'WAst',\n","            'WTO', 'WStl', 'WBlk', 'WPF',\n","            'WTSP', 'WEFGp', 'W3PTAr',\n","            'WFTAr', 'WORp', 'WDRp', 'WTR', 'WPoss',\n","            'WOffRtg', 'WDefRtg', 'WNetRtg',\n","            'WNetRtg', 'WOffEff', 'WDefEff',\n","            'WNetEff', 'WPointDiff', 'WPace',\n","            'WPtsPer100', 'WAr', 'WTOr',\n","            'WATRatio', 'WELO', 'WPythagoreanExp1', \n","            'WPythagoreanExp2']\n","\n","lose_cols = ['Season', 'LScore',\n","            'LFGM', 'LFGA', 'LFGp',\n","            'LFGM3', 'LFGA3', 'LFGp3', 'LFTM',\n","            'LFTA', 'LOR', 'LDR', 'LAst',\n","            'LTO', 'LStl', 'LBlk', 'LPF',\n","            'LTSP', 'LEFGp', 'L3PTAr',\n","            'LFTAr', 'LORp', 'LDRp', 'WTR', 'LPoss',\n","            'LOffRtg', 'LDefRtg', 'LNetRtg',\n","            'LNetRtg', 'LOffEff', 'LDefEff',\n","            'LNetEff', 'LPointDiff', 'LPace',\n","            'LPtsPer100', 'LAr', 'LTOr',\n","            'LATRatio', 'LELO', 'LPythagoreanExp1', \n","            'LPythagoreanExp2']\n","\n","# Generate input and classification matrices and add them to n_data_dict given n\n","def generate_X_y(n):\n","\n","  data = n_data_dict[n][0]\n","\n","  X = pd.DataFrame(columns=cols)\n","  X_wins = data[win_cols].copy()\n","  X_losses = data[lose_cols].copy()\n","\n","  X_wins.columns = cols\n","  X_losses.columns = cols\n","\n","  ones = [1] * len(X_wins)\n","  y_wins = list(zip(X_wins['Season'], ones))\n","  zeros = [0] * len(X_losses)\n","  y_losses = list(zip(X_wins['Season'], zeros))\n","  y_list = np.concatenate((y_wins, y_losses), axis=0)\n","\n","  X = pd.concat([X_wins.reset_index(drop=True), X_losses])\n","  y = pd.DataFrame(y_list, columns=['Season', 'WinOrLoss'])\n","\n","  X_train = X.loc[X['Season'] != 2023]\n","  X_train = X_train.drop(['Season'], axis=1)\n","  y_train = y.loc[y['Season'] != 2023]\n","  y_train = y_train.drop(['Season'], axis=1)\n","  y_train = y_train['WinOrLoss'].to_numpy()\n","\n","  n_data_dict[n].append(X_train)\n","  n_data_dict[n].append(y_train)\n","\n","  X_test = X.loc[X['Season'] == 2023]\n","  X_test = X_test.drop(['Season'], axis=1)\n","  y_test = y.loc[y['Season'] == 2023]\n","  y_test = y_test.drop(['Season'], axis=1)\n","  y_test = y_test['WinOrLoss'].to_numpy()\n","\n","  n_data_dict[n].append(X_test)\n","  n_data_dict[n].append(y_test)\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNXgp2mdnrcC"},"outputs":[],"source":["for n in n_data_dict:\n","  generate_X_y(n)\n","  "]},{"cell_type":"markdown","source":["# Determine optimal value of n"],"metadata":{"id":"1AoCQU8yzc7E"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyxCBBLlIGo6"},"outputs":[],"source":["num_stat_combos = 50\n","features = cols[1:]\n","stat_combos = []\n","\n","for i in range(num_stat_combos):\n","  num_features = random.randint(10, 25)\n","  combo = random.sample(features, k=num_features)\n","  stat_combos.append(combo)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hoiSpSFRmyMz"},"outputs":[],"source":["n = {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}\n","\n","accuracies_list = []\n","accuracies_index = 0\n","\n","for stat_list in stat_combos:\n","  # avg accuracies per n for specified stat combo\n","  accuracies_list_n = []\n","  for N in n:\n","\n","    data = n_data_dict[N]\n","    X_train = data[1]\n","    y_train = data[2]\n","\n","    X_train_select = X_train[stat_list]\n","\n","    # average the accuracy over runs of the model within the training data\n","    accuracies = []\n","    for _ in range(50):\n","\n","      X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train_select, y_train, test_size=0.25)\n","\n","      gnb = GaussianNB()\n","      y_pred = gnb.fit(X_train_train, y_train_train).predict(X_train_test)\n","      accuracies.append(1- ((y_train_test != y_pred).sum())/X_train_test.shape[0])\n","    \n","    # print(\"N = {}, stats = {}\".format(N, stat_list))\n","    mean = np.mean(accuracies)\n","    # print(mean)\n","    accuracies_list_n.append(mean)\n","\n","  accuracies_list.append(accuracies_list_n)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kw8EtO38qQAx"},"outputs":[],"source":["n = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n","\n","plt.figure()\n","plt.xlabel(\"Average number of games taken as input (n)\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Women\\'s Random Feature Subset Accuracy per n\"\")\n","plt.xticks(np.linspace(1,15,15))\n","\n","for i in range(len(accuracies_list)):\n","  plt.plot(n, accuracies_list[i])\n","\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3ayks0xaNFi"},"outputs":[],"source":["transposed_acc_list = np.transpose(accuracies_list)\n","\n","avg_acc = []\n","\n","for i in range(len(transposed_acc_list)):\n","  avg_acc.append(np.mean(transposed_acc_list[i]))\n","\n","print(max(avg_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8C3I8zantX-j"},"outputs":[],"source":["plt.figure()\n","plt.xlabel(\"Average number of games taken as input (n)\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Women\\'s Average Random Feature Subset Accuracy per n\")\n","plt.xticks(np.linspace(1,15,15))\n","\n","plt.plot(n, avg_acc)\n","plt.show()"]},{"cell_type":"markdown","source":["Note, model accuracy is maximized at n = 9."],"metadata":{"id":"lICPmiO0yc1o"}},{"cell_type":"markdown","metadata":{"id":"yUsIZV6CrLF0"},"source":["# Create correlation matrix of features for each n based on training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGf8xOUNrasd"},"outputs":[],"source":["corr_matrix_n_list = [9]\n","for n in corr_matrix_n_list:\n","  X_pre_2023 = n_data_dict[n][1]\n","  plt.figure(figsize=(15, 12))\n","  plt.title('Women\\'s Correlation Matrix, n={}'.format(n))\n","  correlation_heatmap = sns.heatmap(X_pre_2023.corr(), annot=True, fmt =\".1f\", annot_kws={\"fontsize\":8})\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"MzX-rKZBtNuz"},"source":["Analysis: as n increases, the magnitude of correlation increases between features."]},{"cell_type":"markdown","metadata":{"id":"bbUgpurXtXUq"},"source":["# Recursive Feature Elimination"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cDGFMonv3MZ"},"outputs":[],"source":["regressor = RandomForestRegressor(n_estimators=100, max_depth=10)\n","num_features_to_select = 1\n","rfe = RFE(regressor, n_features_to_select=num_features_to_select, verbose=3)\n","rfe.fit(n_data_dict[9][1], n_data_dict[9][2])\n","\n","from operator import itemgetter\n","features = n_data_dict[9][1].columns.to_list()\n","for x, y in (sorted(zip(rfe.ranking_ , features), key=itemgetter(0))):\n","    print(x, y)"]},{"cell_type":"code","source":["ranks = list(map(float, rfe.ranking_))\n","order = -1\n","minmax = MinMaxScaler()\n","ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n","ranks = map(lambda x: round(x, 2), ranks)\n","\n","rank_dict = dict(zip(cols[1:], ranks))\n","\n","print(rank_dict)\n","\n","rank_dict = dict(sorted(rank_dict.items(), key=lambda item: item[1], reverse=True))\n","\n","rank_df = pd.DataFrame(list(rank_dict.items()), columns=['Feature', 'RFE Ranking'])\n","\n","fig, ax = plt.subplots(figsize=(8, 8))\n","plt.title('Women\\'s RFE Ranking Plot at n=9')\n","sns.barplot(x=\"RFE Ranking\", y=\"Feature\", data=rank_df, ax=ax, palette=\"flare\")\n","plt.show()"],"metadata":{"id":"2E70IYhJdKyE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PFnmK52SmqCr"},"source":["# Models"]},{"cell_type":"markdown","source":["## SVM"],"metadata":{"id":"z2I7hwon-cYo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Czb8_kZxsA2b"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler \n","from sklearn.svm import SVC # \"Support vector classifier\"  \n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import cross_val_score\n","\n","features_selected = [\"OffRtg\",\"Pace\",\"TRp\",\"PointDiff\"]\n","\n","X_train_selected = n_data_dict[9][1][features_selected]\n","x = X_train_selected.values\n","standard_scaler = StandardScaler()\n","x_scaled = standard_scaler.fit_transform(x)\n","X_train_selected = pd.DataFrame(x_scaled, columns=features_selected)\n","\n","y_train_selected = n_data_dict[9][2]\n","\n","X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train_selected, y_train_selected, test_size=0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUtJOab_II02","executionInfo":{"status":"ok","timestamp":1686113835856,"user_tz":420,"elapsed":613770,"user":{"displayName":"Sahana S","userId":"03031798974449118633"}},"outputId":"0d978b62-85f5-412a-efdd-41d5fe8fd4a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["[LibSVM]SVM accuracy:  0.8313933629364934\n"]}],"source":["svm = SVC(kernel='rbf', gamma='auto', C=100, random_state=0, verbose=3)  \n","y_pred = svm.fit(X_train_train, y_train_train)\n","print(\"SVM accuracy: \", svm.score(X_train_test, y_train_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZ_LwIIz_Tf3","executionInfo":{"status":"ok","timestamp":1686113885454,"user_tz":420,"elapsed":49611,"user":{"displayName":"Sahana S","userId":"03031798974449118633"}},"outputId":"3e93b1f7-5e44-41a9-832f-3440a54dcdea"},"outputs":[{"output_type":"stream","name":"stdout","text":["SVM accuracy on 2023 season:  0.500089253837915\n"]}],"source":["# 2023 data\n","X_test_2023 = n_data_dict[9][3][features_selected]\n","y_test_2023 = n_data_dict[9][4]\n","\n","ree = svm.score(X_test_2023, y_test_2023)\n","print(\"SVM accuracy on 2023 season: \", svm.score(X_test_2023, y_test_2023))\n"]},{"cell_type":"markdown","source":["## GNB"],"metadata":{"id":"JSl3LaQ2-eS7"}},{"cell_type":"code","source":["N = 9\n","data = pd.read_csv(\"/content/drive/MyDrive/Undergrad/Smore Year/Classes/BEM 120/data/avg_matchups_n={}.csv\".format(N), sep=\",\")\n","\n","stat_combos = [[\"OffRtg\",\"Pace\",\"TRp\",\"PointDiff\",\"DefRtg\",\"PythagoreanExp1\",\"NetRtg\"],\n","               [\"OffRtg\",\"Pace\",\"TRp\",\"PointDiff\",\"DefRtg\",\"PythagoreanExp1\",\"NetRtg\",\"ELO\"],\n","               [\"OffRtg\",\"Pace\",\"TRp\",\"PointDiff\",\"DefRtg\",\"PythagoreanExp1\",\"NetRtg\",\"ELO\",\"FGA\"],\n","               [\"OffRtg\",\"Pace\",\"TRp\",\"PointDiff\",\"DefRtg\",\"PythagoreanExp1\",\"NetRtg\",\"ELO\",\"FGp3\"],\n","               [\"NetRtg\",\"Pace\",\"TRp\",\"PythagoreanExp1\",\"FGA\",\"TOr\",\"DefEff\",\"Ar\"],\n","               ['OffRtg', 'Pace', 'TRp', 'PointDiff', 'DefRtg', 'PythagoreanExp1', 'NetRtg', 'ELO', 'FGA'],\n","               [\"NetRtg\",\"ELO\",\"ATRatio\",\"Pace\",\"DefEff\",\"OffEff\",\"DRp\",\"ORp\",\"FTAr\",\"EFGp\"],\n","               [\"OffRtg\",\"DefRtg\",\"ELO\",\"Ar\",\"TOr\",\"ORp\",\"DRp\",\"TRp\",\"TSP\",\"3PTAr\"]]\n","\n","for stat_list in stat_combos:\n","\n","  cols = ['Season', 'Score', 'FGM', \n","          'FGA', 'FGp', 'FGM3', 'FGA3', 'FGp3', 'FTM', \n","          'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl',\n","          'Blk', 'PF', 'TSP', 'EFGp', '3PTAr', 'FTAr', \n","          'ORp', 'DRp', 'TRp', 'TR', 'Poss', 'OffRtg', 'DefRtg',\n","          'NetRtg', 'OffEff', 'DefEff', 'NetEff', \n","          'PointDiff', 'Pace','PtsPer100', 'Ar', 'TOr', \n","          'ATRatio', 'ELO', 'PythagoreanExp1', 'PythagoreanExp2']\n","\n","  X = pd.DataFrame(columns=cols)\n","\n","  X_wins = data[['Season', 'WScore',\n","                        'WFGM', 'WFGA', 'WFGp',\n","                        'WFGM3', 'WFGA3', 'WFGp3', 'WFTM',\n","                        'WFTA', 'WOR', 'WDR', 'WAst',\n","                        'WTO', 'WStl', 'WBlk', 'WPF',\n","                        'WTSP', 'WEFGp', 'W3PTAr',\n","                        'WFTAr', 'WORp', 'WDRp', 'WTR', 'WPoss',\n","                        'WOffRtg', 'WDefRtg', 'WNetRtg',\n","                        'WNetRtg', 'WOffEff', 'WDefEff',\n","                        'WNetEff', 'WPointDiff', 'WPace',\n","                        'WPtsPer100', 'WAr', 'WTOr',\n","                        'WATRatio', 'WELO', 'WPythagoreanExp1', 'WPythagoreanExp2']].copy()\n","  X_wins.columns = cols\n","\n","  X_losses = data[['Season', 'LScore',\n","                        'LFGM', 'LFGA', 'LFGp',\n","                        'LFGM3', 'LFGA3', 'LFGp3', 'LFTM',\n","                        'LFTA', 'LOR', 'LDR', 'LAst',\n","                        'LTO', 'LStl', 'LBlk', 'LPF',\n","                        'LTSP', 'LEFGp', 'L3PTAr',\n","                        'LFTAr', 'LORp', 'LDRp', 'WTR', 'LPoss',\n","                        'LOffRtg', 'LDefRtg', 'LNetRtg',\n","                        'LNetRtg', 'LOffEff', 'LDefEff',\n","                        'LNetEff', 'LPointDiff', 'LPace',\n","                        'LPtsPer100', 'LAr', 'LTOr',\n","                        'LATRatio', 'LELO', 'LPythagoreanExp1', 'LPythagoreanExp2']].copy().reset_index(drop=True)\n","  X_losses.columns = cols\n","\n","  ones = [1] * len(X_wins)\n","  y_wins = list(zip(X_wins['Season'], ones))\n","\n","  zeros = [0] * len(X_losses)\n","  y_losses = list(zip(X_wins['Season'], zeros))\n","\n","  y_list = np.concatenate((y_wins, y_losses), axis=0)\n","\n","  # X -> training data\n","  # y -> classification (1 for win, 0 for loss)\n","  X = pd.concat([X_wins.reset_index(drop=True), X_losses])\n","  y = pd.DataFrame(y_list, columns=['Season', 'WinOrLoss'])\n","\n","  # data before 2023 is training split, 2023 is testing split\n","  X_train = X.loc[X['Season'] != 2023]\n","  X_train = X_train.drop(['Season'], axis=1)\n","\n","  X_test = X.loc[X['Season'] == 2023]\n","  X_test = X_test.drop(['Season'], axis=1)\n","\n","  y_train = y.loc[y['Season'] != 2023]\n","  y_train = y_train.drop(['Season'], axis=1)\n","  y_train = y_train['WinOrLoss'].to_numpy()\n","\n","  y_test = y.loc[y['Season'] == 2023]\n","  y_test = y_test.drop(['Season'], axis=1)\n","  y_test = y_test['WinOrLoss'].to_numpy()\n","\n","  X_train_select = X_train[stat_list]\n","  X_test_select = X_test[stat_list]\n","\n","  # average the accuracy over runs of the model within the training data\n","  accuracies = []\n","  for _ in range(50):\n","\n","    X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train_select, y_train, test_size=0.25)\n","\n","    gnb = GaussianNB()\n","    y_pred = gnb.fit(X_train_train, y_train_train).predict(X_train_test)\n","    accuracies.append(1- ((y_train_test != y_pred).sum())/X_train_test.shape[0])\n","  \n","  print(\"stats = {}\".format(stat_list))\n","  print(np.mean(accuracies))\n","  y_test_pred = gnb.fit(X_train_select, y_train).predict(X_test_select)\n","  print(1- ((y_test != y_test_pred).sum())/X_test.shape[0])"],"metadata":{"id":"tjKaBvcE-ulF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686178140659,"user_tz":420,"elapsed":21341,"user":{"displayName":"Katherine Marquis","userId":"17438712515933825923"}},"outputId":"601da649-26ae-449a-ac19-826afa269629"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["stats = ['OffRtg', 'Pace', 'TRp', 'PointDiff', 'DefRtg', 'PythagoreanExp1', 'NetRtg']\n","0.6344476829867366\n","0.6322106438407146\n","stats = ['OffRtg', 'Pace', 'TRp', 'PointDiff', 'DefRtg', 'PythagoreanExp1', 'NetRtg', 'ELO']\n","0.6358113640085148\n","0.6296055080014886\n","stats = ['OffRtg', 'Pace', 'TRp', 'PointDiff', 'DefRtg', 'PythagoreanExp1', 'NetRtg', 'ELO', 'FGA']\n","0.6358166038971672\n","0.6299776702642352\n","stats = ['OffRtg', 'Pace', 'TRp', 'PointDiff', 'DefRtg', 'PythagoreanExp1', 'NetRtg', 'ELO', 'FGp3']\n","0.6354753561486818\n","0.6320245627093413\n","stats = ['NetRtg', 'Pace', 'TRp', 'PythagoreanExp1', 'FGA', 'TOr', 'DefEff', 'Ar']\n","0.6323739970525627\n","0.6266282098995162\n","stats = ['OffRtg', 'Pace', 'TRp', 'PointDiff', 'DefRtg', 'PythagoreanExp1', 'NetRtg', 'ELO', 'FGA']\n","0.63578385459309\n","0.6299776702642352\n","stats = ['NetRtg', 'ELO', 'ATRatio', 'Pace', 'DefEff', 'OffEff', 'DRp', 'ORp', 'FTAr', 'EFGp']\n","0.6312212215490421\n","0.6313732787495347\n","stats = ['OffRtg', 'DefRtg', 'ELO', 'Ar', 'TOr', 'ORp', 'DRp', 'TRp', 'TSP', '3PTAr']\n","0.6308354347470116\n","0.6317454410122814\n"]}]},{"cell_type":"markdown","source":["## DT"],"metadata":{"id":"gkyCteZ7-vl8"}},{"cell_type":"code","source":["N = 9\n","data = pd.read_csv(\"/content/drive/MyDrive/Undergrad/Smore Year/Classes/BEM 120/data/avg_matchups_n={}.csv\".format(N), sep=\",\")\n","\n","stat_combos = [[\"OffRtg\",\"Pace\",\"TRp\",\"PointDiff\",\"DefRtg\",\"PythagoreanExp1\",\"NetRtg\"],\n","               [\"OffRtg\",\"Pace\",\"TRp\",\"PointDiff\",\"DefRtg\",\"PythagoreanExp1\",\"NetRtg\",\"ELO\"],\n","               [\"OffRtg\",\"Pace\",\"TRp\",\"PointDiff\",\"DefRtg\",\"PythagoreanExp1\",\"NetRtg\",\"ELO\",\"FGA\"],\n","               [\"OffRtg\",\"Pace\",\"TRp\",\"PointDiff\",\"DefRtg\",\"PythagoreanExp1\",\"NetRtg\",\"ELO\",\"FGp3\"],\n","               [\"NetRtg\",\"Pace\",\"TRp\",\"PythagoreanExp1\",\"FGA\",\"TOr\",\"DefEff\",\"Ar\"],\n","               ['OffRtg', 'Pace', 'TRp', 'PointDiff', 'DefRtg', 'PythagoreanExp1', 'NetRtg', 'ELO', 'FGA'],\n","               [\"NetRtg\",\"ELO\",\"ATRatio\",\"Pace\",\"DefEff\",\"OffEff\",\"DRp\",\"ORp\",\"FTAr\",\"EFGp\"],\n","               [\"OffRtg\",\"DefRtg\",\"ELO\",\"Ar\",\"TOr\",\"ORp\",\"DRp\",\"TRp\",\"TSP\",\"3PTAr\"]]\n","\n","for stat_list in stat_combos:\n","\n","  cols = ['Season', 'Score', 'FGM', \n","          'FGA', 'FGp', 'FGM3', 'FGA3', 'FGp3', 'FTM', \n","          'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl',\n","          'Blk', 'PF', 'TSP', 'EFGp', '3PTAr', 'FTAr', \n","          'ORp', 'DRp', 'TRp', 'TR', 'Poss', 'OffRtg', 'DefRtg',\n","          'NetRtg', 'OffEff', 'DefEff', 'NetEff', \n","          'PointDiff', 'Pace','PtsPer100', 'Ar', 'TOr', \n","          'ATRatio', 'ELO', 'PythagoreanExp1', 'PythagoreanExp2']\n","\n","  X = pd.DataFrame(columns=cols)\n","\n","  X_wins = data[['Season', 'WScore',\n","                        'WFGM', 'WFGA', 'WFGp',\n","                        'WFGM3', 'WFGA3', 'WFGp3', 'WFTM',\n","                        'WFTA', 'WOR', 'WDR', 'WAst',\n","                        'WTO', 'WStl', 'WBlk', 'WPF',\n","                        'WTSP', 'WEFGp', 'W3PTAr',\n","                        'WFTAr', 'WORp', 'WDRp', 'WTR', 'WPoss',\n","                        'WOffRtg', 'WDefRtg', 'WNetRtg',\n","                        'WNetRtg', 'WOffEff', 'WDefEff',\n","                        'WNetEff', 'WPointDiff', 'WPace',\n","                        'WPtsPer100', 'WAr', 'WTOr',\n","                        'WATRatio', 'WELO', 'WPythagoreanExp1', 'WPythagoreanExp2']].copy()\n","  X_wins.columns = cols\n","\n","  X_losses = data[['Season', 'LScore',\n","                        'LFGM', 'LFGA', 'LFGp',\n","                        'LFGM3', 'LFGA3', 'LFGp3', 'LFTM',\n","                        'LFTA', 'LOR', 'LDR', 'LAst',\n","                        'LTO', 'LStl', 'LBlk', 'LPF',\n","                        'LTSP', 'LEFGp', 'L3PTAr',\n","                        'LFTAr', 'LORp', 'LDRp', 'WTR', 'LPoss',\n","                        'LOffRtg', 'LDefRtg', 'LNetRtg',\n","                        'LNetRtg', 'LOffEff', 'LDefEff',\n","                        'LNetEff', 'LPointDiff', 'LPace',\n","                        'LPtsPer100', 'LAr', 'LTOr',\n","                        'LATRatio', 'LELO', 'LPythagoreanExp1', 'LPythagoreanExp2']].copy().reset_index(drop=True)\n","  X_losses.columns = cols\n","\n","  ones = [1] * len(X_wins)\n","  y_wins = list(zip(X_wins['Season'], ones))\n","\n","  zeros = [0] * len(X_losses)\n","  y_losses = list(zip(X_wins['Season'], zeros))\n","\n","  y_list = np.concatenate((y_wins, y_losses), axis=0)\n","\n","  # X -> training data\n","  # y -> classification (1 for win, 0 for loss)\n","  X = pd.concat([X_wins.reset_index(drop=True), X_losses])\n","  y = pd.DataFrame(y_list, columns=['Season', 'WinOrLoss'])\n","\n","  # data before 2023 is training split, 2023 is testing split\n","  X_train = X.loc[X['Season'] != 2023]\n","  X_train = X_train.drop(['Season'], axis=1)\n","\n","  X_test = X.loc[X['Season'] == 2023]\n","  X_test = X_test.drop(['Season'], axis=1)\n","\n","  y_train = y.loc[y['Season'] != 2023]\n","  y_train = y_train.drop(['Season'], axis=1)\n","  y_train = y_train['WinOrLoss'].to_numpy()\n","\n","  y_test = y.loc[y['Season'] == 2023]\n","  y_test = y_test.drop(['Season'], axis=1)\n","  y_test = y_test['WinOrLoss'].to_numpy()\n","\n","  X_train_select = X_train[stat_list]\n","  X_test_select = X_test[stat_list]\n","\n","  # average the accuracy over runs of the model within the training data\n","  accuracies = []\n","  for _ in range(50):\n","\n","    X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train_select, y_train, test_size=0.25)\n","\n","    model = tree.DecisionTreeClassifier(random_state=5)\n","    y_pred = model.fit(X_train_train, y_train_train).predict(X_train_test)\n","    accuracies.append(1- ((y_train_test != y_pred).sum())/X_train_test.shape[0])\n","  \n","  print(\"stats = {}\".format(stat_list))\n","  print(np.mean(accuracies))\n","  y_test_pred = model.fit(X_train_select, y_train).predict(X_test_select)\n","  print(1- ((y_test != y_test_pred).sum())/X_test.shape[0])"],"metadata":{"id":"CzKbAErnDYlo"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}